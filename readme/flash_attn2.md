# Falsh Attention 2.0
We're thrilled to announce that LMFlow now supports training and inference using flash attn2! This cutting-edge feature will take your language modeling to the next level. To use it, simply add ``` --use_flash_attention True ``` to the corresponding bash script.

But that's not all - we're also excited to share that LMFlow now supports GPU models A40 and A100! This means faster and more efficient training for your models.

And as if that wasn't enough, we've also expanded our supported model architectures to include the powerful ["LlamaForCausalLM", "GPTNeoForCausalLM", "BloomForCausalLM"]. With LMFlow, the possibilities for language modeling are endless.

Upgrade to LMFlow now and experience the future of language modeling!